{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16688235-75e1-4ceb-8170-c6c5e6bd90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this new scraper works, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db1ab5d-7863-4156-aae4-ec54642ed3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the company website URL:  https://www.tcs.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import hashlib\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Suppress SSL certificate warnings (use with caution)\n",
    "requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    \"\"\"Creates a folder if it doesn't exist.\"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "def get_unique_filename(url):\n",
    "    \"\"\"Generates a unique filename based on URL hash.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    file_name = hashlib.md5(url.encode()).hexdigest()\n",
    "    return file_name + \".html\"\n",
    "\n",
    "def get_document_filename(url):\n",
    "    \"\"\"Extracts the filename from the URL for documents.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    return os.path.basename(parsed_url.path)\n",
    "\n",
    "def save_content(content, folder_path, filename):\n",
    "    \"\"\"Saves content (text or binary) to a file.\"\"\"\n",
    "    with open(os.path.join(folder_path, filename), 'wb') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def scrape_website(url, folder_path, visited=set(), depth=0, max_depth=3, common_tabs=None):\n",
    "    \"\"\"Recursively scrapes a website up to max_depth.\"\"\"\n",
    "    if depth > max_depth or url in visited:\n",
    "        return\n",
    "\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        if common_tabs:\n",
    "            for tab in common_tabs:\n",
    "                for elem in soup.select(tab):\n",
    "                    elem.decompose()\n",
    "\n",
    "        # Save HTML content excluding header, footer, and common tabs\n",
    "        html_folder = os.path.join(folder_path, 'htmls')\n",
    "        create_folder(html_folder)\n",
    "        filtered_content = soup.prettify('utf-8')\n",
    "        save_content(filtered_content, html_folder, get_unique_filename(url))\n",
    "\n",
    "        # Save documents\n",
    "        doc_folder = os.path.join(folder_path, 'documents')\n",
    "        create_folder(doc_folder)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            full_url = urljoin(url, href)\n",
    "            if urlparse(full_url).netloc == urlparse(url).netloc:\n",
    "                if full_url.endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx')):\n",
    "                    save_content(requests.get(full_url, verify=False).content, doc_folder, get_document_filename(full_url))\n",
    "                else:\n",
    "                    scrape_website(full_url, folder_path, visited, depth + 1, max_depth, common_tabs)\n",
    "                    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "def safe_filename(text):\n",
    "    # Replace characters not allowed in Windows filenames\n",
    "    return ''.join(c for c in text if c.isalnum() or c in [' ', '_', '-', '.'])\n",
    "\n",
    "def scrape_careers_section(company_url):\n",
    "    # Parse the company URL\n",
    "    parsed_url = urlparse(company_url)\n",
    "    company_name = parsed_url.netloc.replace('www.', '').replace('.com', '').replace('.org', '')  # Extract company name\n",
    "\n",
    "    # Create main directory for scraped data\n",
    "    main_dir = f\"{company_name}_careers_scraped_data\"\n",
    "    if not os.path.exists(main_dir):\n",
    "        os.makedirs(main_dir)\n",
    "\n",
    "    # Subdirectories for links and documents\n",
    "    links_dir = os.path.join(main_dir, 'links')\n",
    "    if not os.path.exists(links_dir):\n",
    "        os.makedirs(links_dir)\n",
    "\n",
    "    documents_dir = os.path.join(main_dir, 'documents')\n",
    "    if not os.path.exists(documents_dir):\n",
    "        os.makedirs(documents_dir)\n",
    "\n",
    "    # Retry mechanism setup\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "\n",
    "    # Fetch HTML content\n",
    "    try:\n",
    "        response = session.get(company_url, timeout=30, allow_redirects=False)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find link to Careers section\n",
    "            careers_link = None\n",
    "            header_careers_link = soup.find('a', string='Careers')\n",
    "            footer_careers_link = soup.find('a', string='Careers')  # Adjust if different in actual site structure\n",
    "\n",
    "            if header_careers_link:\n",
    "                careers_link = header_careers_link.get('href')\n",
    "            elif footer_careers_link:\n",
    "                careers_link = footer_careers_link.get('href')\n",
    "\n",
    "            if careers_link:\n",
    "                # Convert relative URL to absolute URL\n",
    "                careers_url = urljoin(company_url, careers_link)\n",
    "                careers_response = session.get(careers_url, timeout=30, allow_redirects=False)\n",
    "                if careers_response.status_code == 200:\n",
    "                    careers_soup = BeautifulSoup(careers_response.content, 'html.parser')\n",
    "\n",
    "                    # Example: Find all links in the 'Careers' section\n",
    "                    links = careers_soup.find_all('a', href=True)\n",
    "                    for link in links:\n",
    "                        link_url = urljoin(careers_url, link['href'])  # Convert relative link to absolute\n",
    "                        # Check if link is HTTP/HTTPS\n",
    "                        if link_url.startswith('http://') or link_url.startswith('https://'):\n",
    "                            # Save each link as an HTML file\n",
    "                            filename = safe_filename(link_url.split('/')[-1].replace('.html', '')) + '.html'\n",
    "                            with open(os.path.join(links_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                                f.write(session.get(link_url, timeout=30, allow_redirects=False).text)\n",
    "\n",
    "                    # Example: Find all downloadable documents (PDFs)\n",
    "                    documents = careers_soup.find_all('a', {'class': 'document'}, href=True)\n",
    "                    for doc in documents:\n",
    "                        doc_url = urljoin(careers_url, doc['href'])  # Convert relative link to absolute\n",
    "                        # Check if link is HTTP/HTTPS\n",
    "                        if doc_url.startswith('http://') or doc_url.startswith('https://'):\n",
    "                            # Download and save documents\n",
    "                            doc_filename = safe_filename(doc_url.split('/')[-1])\n",
    "                            with open(os.path.join(documents_dir, doc_filename), 'wb') as f:\n",
    "                                f.write(session.get(doc_url, timeout=30, allow_redirects=False).content)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve Careers page: {careers_response.status_code}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Careers link not found.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    company_url = input(\"Enter the company website URL: \")\n",
    "    parsed_url = urlparse(company_url)\n",
    "    domain = parsed_url.netloc.split('.')[1] if 'www' in parsed_url.netloc else parsed_url.netloc.split('.')[0]\n",
    "    output_folder = f'{domain}_scraped_data'\n",
    "    common_tabs = ['#main-nav', '.footer', 'header', 'footer']\n",
    "\n",
    "    create_folder(output_folder)\n",
    "    scrape_website(company_url, output_folder, common_tabs=common_tabs)\n",
    "    scrape_careers_section(company_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549f0b8-ca52-4175-a05e-a259336bb076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
