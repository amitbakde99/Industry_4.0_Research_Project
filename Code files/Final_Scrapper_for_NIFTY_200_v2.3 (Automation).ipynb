{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":60711,"status":"ok","timestamp":1723540485442,"user":{"displayName":"Amit","userId":"08897145000475495924"},"user_tz":-330},"id":"kaZ5qEWAFB4d","outputId":"0122cd73-e6ec-4fda-81b2-7f2198e622bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting requests-toolbelt\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from requests-toolbelt) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (2024.7.4)\n","Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: requests-toolbelt\n","Successfully installed requests-toolbelt-1.0.0\n","Collecting pdfminer.six\n","  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.8)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n","Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pdfminer.six\n","Successfully installed pdfminer.six-20240706\n","Collecting PyMuPDF\n","  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n","  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n","Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n","Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n","Collecting python-docx\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n","Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-docx\n","Successfully installed python-docx-1.1.2\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"]}],"source":["#!pip install nltk\n","#!pip install requests\n","#!pip install beautifulsoup4\n","#!pip install urllib3\n","!pip install requests-toolbelt\n","!pip install pdfminer.six\n","!pip install PyMuPDF\n","!pip install PyPDF2\n","!pip install python-docx\n","!pip install tqdm\n","!pip install openpyxl"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4145,"status":"ok","timestamp":1723540489580,"user":{"displayName":"Amit","userId":"08897145000475495924"},"user_tz":-330},"id":"nofSlwPbEuPC","outputId":"16283fe8-7feb-4dbd-90d5-15b5531b0230"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import os\n","import shutil\n","import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin, urlparse\n","import hashlib\n","from urllib3.exceptions import InsecureRequestWarning\n","from requests.adapters import HTTPAdapter\n","from requests.packages.urllib3.util.retry import Retry\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from pdfminer.high_level import extract_text\n","\n","# Ensure necessary NLTK data packages are downloaded\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1723540489580,"user":{"displayName":"Amit","userId":"08897145000475495924"},"user_tz":-330},"id":"BElOoWpMEl7O"},"outputs":[],"source":["\n","###---------------------------------------------------------------scraper-------------------------------------------------\n","\n","# Suppress SSL certificate warnings (use with caution)\n","requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)\n","\n","def create_folder(folder_path):\n","    \"\"\"Creates a folder if it doesn't exist.\"\"\"\n","    os.makedirs(folder_path, exist_ok=True)\n","\n","def get_unique_filename(url):\n","    \"\"\"Generates a unique filename based on URL hash.\"\"\"\n","    parsed_url = urlparse(url)\n","    file_name = hashlib.md5(url.encode()).hexdigest()\n","    return file_name + \".html\"\n","\n","def get_document_filename(url):\n","    \"\"\"Extracts the filename from the URL for documents.\"\"\"\n","    parsed_url = urlparse(url)\n","    return os.path.basename(parsed_url.path)\n","\n","def save_content(content, folder_path, filename):\n","    \"\"\"Saves content (text or binary) to a file.\"\"\"\n","    with open(os.path.join(folder_path, filename), 'wb') as f:\n","        f.write(content)\n","\n","def scrape_website(url, folder_path, visited=set(), depth=0, max_depth=3, common_tabs=None):\n","    \"\"\"Recursively scrapes a website up to max_depth.\"\"\"\n","    if depth > max_depth or url in visited:\n","        return\n","\n","    visited.add(url)\n","\n","    try:\n","        response = requests.get(url, verify=False)\n","        response.raise_for_status()\n","\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        if common_tabs:\n","            for tab in common_tabs:\n","                for elem in soup.select(tab):\n","                    elem.decompose()\n","\n","        # Save HTML content excluding header, footer, and common tabs\n","        html_folder = os.path.join(folder_path, 'htmls')\n","        create_folder(html_folder)\n","        filtered_content = soup.prettify('utf-8')\n","        save_content(filtered_content, html_folder, get_unique_filename(url))\n","\n","        # Save documents\n","        doc_folder = os.path.join(folder_path, 'documents')\n","        create_folder(doc_folder)\n","        for link in soup.find_all('a', href=True):\n","            href = link.get('href')\n","            full_url = urljoin(url, href)\n","            if urlparse(full_url).netloc == urlparse(url).netloc:\n","                if full_url.endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx')):\n","                    save_content(requests.get(full_url, verify=False).content, doc_folder, get_document_filename(full_url))\n","                else:\n","                    scrape_website(full_url, folder_path, visited, depth + 1, max_depth, common_tabs)\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching {url}: {e}\")\n","\n","def safe_filename(text):\n","    # Replace characters not allowed in Windows filenames\n","    return ''.join(c for c in text if c.isalnum() or c in [' ', '_', '-', '.'])\n","\n","def scrape_careers_section(company_url):\n","    # Parse the company URL\n","    parsed_url = urlparse(company_url)\n","    company_name = parsed_url.netloc.replace('www.', '').replace('.com', '').replace('.org', '')  # Extract company name\n","\n","    # Create main directory for scraped data\n","    main_dir = f\"{company_name}_scraped_data\"\n","    if not os.path.exists(main_dir):\n","        os.makedirs(main_dir)\n","\n","    careers_dir = os.path.join(main_dir, f\"{company_name}_careers_scraped_data\")\n","    if not os.path.exists(careers_dir):\n","        os.makedirs(careers_dir)\n","\n","    # Retry mechanism setup\n","    session = requests.Session()\n","    retry_strategy = Retry(\n","        total=3,\n","        status_forcelist=[429, 500, 502, 503, 504],\n","        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n","    )\n","    adapter = HTTPAdapter(max_retries=retry_strategy)\n","    session.mount(\"https://\", adapter)\n","    session.mount(\"http://\", adapter)\n","\n","    # Fetch HTML content\n","    try:\n","        response = session.get(company_url, timeout=30, allow_redirects=False)\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","\n","            # Find link to Careers section\n","            careers_link = None\n","            header_careers_link = soup.find('a', string='Careers')\n","            footer_careers_link = soup.find('a', string='Careers')  # Adjust if different in actual site structure\n","\n","            if header_careers_link:\n","                careers_link = header_careers_link.get('href')\n","            elif footer_careers_link:\n","                careers_link = footer_careers_link.get('href')\n","\n","            if careers_link:\n","                # Convert relative URL to absolute URL\n","                careers_url = urljoin(company_url, careers_link)\n","                careers_response = session.get(careers_url, timeout=30, allow_redirects=False)\n","                if careers_response.status_code == 200:\n","                    careers_soup = BeautifulSoup(careers_response.content, 'html.parser')\n","\n","                    # Example: Find all links in the 'Careers' section\n","                    links = careers_soup.find_all('a', href=True)\n","                    for link in links:\n","                        link_url = urljoin(careers_url, link['href'])  # Convert relative link to absolute\n","                        # Check if link is HTTP/HTTPS\n","                        if link_url.startswith('http://') or link_url.startswith('https://'):\n","                            # Save each link as an HTML file\n","                            filename = safe_filename(link_url.split('/')[-1].replace('.html', '')) + '.html'\n","                            with open(os.path.join(careers_dir, filename), 'w', encoding='utf-8') as f:\n","                                f.write(session.get(link_url, timeout=30, allow_redirects=False).text)\n","\n","                    # Example: Find all downloadable documents (PDFs)\n","                    documents = careers_soup.find_all('a', {'class': 'document'}, href=True)\n","                    for doc in documents:\n","                        doc_url = urljoin(careers_url, doc['href'])  # Convert relative link to absolute\n","                        # Check if link is HTTP/HTTPS\n","                        if doc_url.startswith('http://') or doc_url.startswith('https://'):\n","                            # Download and save documents\n","                            doc_filename = safe_filename(doc_url.split('/')[-1])\n","                            with open(os.path.join(careers_dir, doc_filename), 'wb') as f:\n","                                f.write(session.get(doc_url, timeout=30, allow_redirects=False).content)\n","                else:\n","                    print(f\"Failed to retrieve Careers page: {careers_response.status_code}\")\n","\n","            else:\n","                print(\"Careers link not found.\")\n","\n","        else:\n","            print(f\"Failed to retrieve page: {response.status_code}\")\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching page: {e}\")\n","\n","#---------------------------------------------------------document_combiner-----------------------------------------------------------\n","\n","def extract_text_from_pdfs(folder_path):\n","    combined_text = \"\"\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.pdf'):\n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                text = extract_text(file_path)\n","                combined_text += text\n","            except Exception as e:\n","                print(f\"Error reading {filename}: {e}\")\n","\n","    return combined_text\n","\n","def process_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Remove special characters using regex\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Tokenize the text\n","    words = nltk.word_tokenize(text)\n","\n","    company_stopwords = [\n","        \"download\", \"www\", \"html\", \"http\", \"login\", \"menu\", \"chat\", \"article\", \"disclaimer\", \"facebook\", \"cart\", \"loading\",\n","        \"click\", \"com\", \"htm\", \"https\", \"logout\", \"navbar\", \"message\", \"blog\", \"copyright\", \"twitter\", \"checkout\",\n","        \"processing\", \"submit\", \"org\", \"php\", \"ftp\", \"register\", \"footer\", \"reply\", \"post\", \"terms\", \"instagram\", \"order\",\n","        \"waiting\", \"login\", \"net\", \"asp\", \"mailto\", \"signup\", \"sidebar\", \"comment\", \"news\", \"privacy\", \"linkedin\",\n","        \"invoice\", \"error\", \"logout\", \"gov\", \"jsp\", \"tel\", \"signin\", \"header\", \"post\", \"story\", \"policy\", \"pinterest\",\n","        \"billing\", \"success\", \"register\", \"edu\", \"css\", \"news\", \"signout\", \"banner\", \"thread\", \"update\", \"conditions\",\n","        \"youtube\", \"shipping\", \"failure\", \"sign up\", \"co\", \"js\", \"irc\", \"user\", \"ad\", \"forum\", \"headline\", \"agreement\",\n","        \"vimeo\", \"payment\", \"retry\", \"sign in\", \"io\", \"json\",\"file\", \"account\", \"advertisement\", \"discussion\", \"media\",\n","        \"license\", \"flickr\", \"subscribe\", \"refresh\", \"sign out\", \"uk\", \"xml\", \"profile\", \"promo\", \"like\",\n","        \"video\", \"cookie\", \"reddit\", \"membership\", \"reload\", \"contact\", \"de\", \"pdf\", \"href\", \"admin\", \"button\",\n","        \"share\", \"image\", \"settings\", \"tumblr\", \"account\", \"redirect\", \"about\", \"jp\", \"doc\", \"src\", \"dashboard\",\n","        \"click\", \"follow\", \"photo\", \"preferences\", \"snapchat\", \"profile\", \"navigate\", \"home\", \"fr\", \"docx\", \"ref\",\n","        \"settings\", \"read more\", \"subscribe\", \"gallery\", \"options\", \"tiktok\", \"wishlist\", \"submit\", \"menu\", \"au\",\n","        \"xls\", \"utm_source\", \"search\", \"more info\", \"unsubscribe\", \"slideshow\", \"tools\", \"whatsapp\", \"product\", \"validate\",\n","        \"search\", \"ca\", \"xlsx\", \"utm_medium\", \"results\", \"next\", \"notification\", \"podcast\", \"utilities\", \"telegram\",\n","        \"service\", \"authenticate\", \"next\", \"us\", \"ppt\", \"utm_campaign\", \"help\", \"previous\", \"alert\", \"episode\",\n","        \"resources\", \"messenger\", \"pricing\", \"authorize\", \"previous\", \"in\", \"pptx\", \"utm_term\", \"support\", \"back\",\n","        \"update\", \"stream\", \"links\", \"skype\", \"offer\", \"encrypt\", \"back\", \"cn\", \"txt\", \"utm_content\", \"about\", \"top\",\n","        \"upload\", \"broadcast\", \"map\", \"discord\", \"discount\", \"help\", \"br\", \"zip\", \"param\", \"home\", \"bottom\", \"attachment\",\n","        \"channel\", \"navigation\", \"signal\", \"coupon\", \"support\", \"es\", \"rar\", \"sid\", \"news\", \"skip\", \"link\", \"playlist\",\n","        \"sitemap\", \"medium\", \"gift\", \"terms\", \"tar\", \"id\", \"blog\", \"submit\",\"url\", \"archive\", \"blogspot\", \"buy\",\n","        \"conditions\", \"gz\", \"key\", \"post\", \"reset\", \"address\", \"library\", \"wordpress\", \"sell\", \"privacy\", \"exe\", \"token\",\n","        \"article\", \"cancel\", \"contact\", \"resource\", \"github\", \"rent\", \"policy\", \"dmg\", \"hash\", \"category\", \"edit\",\n","        \"phone\", \"bitbucket\", \"lease\", \"disclaimer\", \"iso\", \"index\", \"tag\", \"delete\", \"email\", \"stackoverflow\", \"booking\",\n","        \"sitemap\", \"bin\", \"page\", \"archive\", \"save\", \"support\", \"quora\", \"reservation\", \"faq\", \"img\", \"sort\", \"year\",\n","        \"print\", \"help\", \"meetup\", \"feedback\", \"filter\", \"month\", \"close\", \"faq\", \"eventbrite\", \"news\", \"author\",\n","        \"collapse\", \"tutorial\", \"blog\", \"faq\", \"dropdown\", \"documentation\", \"post\", \"terms\", \"form\", \"manual\",\n","        \"article\", \"privacy\", \"field\", \"report\", \"read more\", \"policy\", \"checkbox\", \"feedback\", \"follow us\",\n","        \"conditions\", \"radio\", \"survey\", \"share\", \"disclaimer\", \"select\", \"like\", \"sitemap\", \"option\",\n","        \"comment\", \"feedback\", \"input\", \"subscribe\", \"gallery\", \"text area\", \"unsubscribe\", \"media\", \"captcha\",\n","        \"learn more\", \"video\", \"view details\", \"image\", \"all rights reserved\", \"photo\", \"© (copyright)\", \"download\",\n","        \"trademark\", \"upload\", \"update\", \"file\", \"settings\", \"attachment\", \"profile\", \"resource\", \"account\", \"link\",\n","        \"admin\", \"share\", \"dashboard\", \"my account\", \"your account\", \"preferences\", \"notifications\", \"messages\", \"inbox\",\n","        \"outbox\", \"send\", \"receive\", \"cart\", \"checkout\", \"order\", \"payment\", \"invoice\", \"billing\", \"shipping\", \"address\",\n","        \"terms of service\", \"conditions of use\", \"user agreement\", \"cookies\", \"advertisement\", \"sponsor\", \"partnership\",\n","        \"careers\", \"jobs\", \"vacancies\", \"apply now\", \"application\", \"newsletter\", \"updates\", \"events\", \"calendar\",\n","        \"press\", \"release\", \"media\", \"gallery\", \"videos\", \"photos\", \"terms & conditions\", \"privacy & policy\", \"contact us\",\n","        \"back to top\", \"accessibility\", \"languages\", \"select language\", \"international\", \"mobile\", \"desktop\"\n","    ]\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    # Update the stopwords list with company-specific stopwords\n","    stop_words.update(company_stopwords)\n","\n","    # Remove stopwords\n","    words = [word for word in words if word not in stop_words]\n","\n","    # Lemmatize the words\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    return ' '.join(lemmatized_words)\n","\n","def process_pdfs(documents_folder, output_file):\n","    # Extract text from PDFs\n","    combined_text = extract_text_from_pdfs(documents_folder)\n","\n","    # Process the text\n","    processed_text = process_text(combined_text)\n","\n","    # Write the processed text to the output file\n","    with open(output_file, 'w', encoding='utf-8') as file:\n","        file.write(processed_text)\n","\n","    print(f\"Processed text saved to {output_file}\")\n","\n","##---------------------------------------------------------------html_combiner----------------------------------------------------------\n","\n","def extract_main_content(html_content):\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","\n","    # Remove header, footer, nav, script, style, and noscript elements\n","    for element in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):\n","        element.decompose()\n","\n","    # Find the main content area\n","    main_content = \"\"\n","    article = soup.find('article')\n","    if article:\n","        main_content = article.get_text(separator='\\n', strip=True)\n","    else:\n","        main = soup.find('main')\n","        if main:\n","            main_content = main.get_text(separator='\\n', strip=True)\n","        else:\n","            large_divs = soup.find_all('div')\n","            for div in large_divs:\n","                if len(div.get_text(strip=True)) > 200:  # Threshold length to identify large content blocks\n","                    main_content += div.get_text(separator='\\n', strip=True) + '\\n'\n","            if not main_content:  # Fallback if no large divs found\n","                body = soup.find('body')\n","                if body:\n","                    main_content = body.get_text(separator='\\n', strip=True)\n","\n","    return main_content\n","\n","def save_to_text_file(content, file_path):\n","    with open(file_path, 'w', encoding='utf-8') as file:\n","        file.write(content)\n","\n","def process_html_files_in_directory(directory_path, output_file_path):\n","    combined_content = \"\"\n","\n","    # Iterate through each file in the directory\n","    for filename in os.listdir(directory_path):\n","        if filename.endswith('.html'):\n","            file_path = os.path.join(directory_path, filename)\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                html_content = file.read()\n","\n","            # Extract the main content\n","            main_content = extract_main_content(html_content)\n","            combined_content += main_content + \"\\n\\n\"\n","\n","    # Save the combined content to a single text file\n","    save_to_text_file(combined_content, output_file_path)\n","    print(f'All content extracted and combined into {output_file_path}')\n","\n","    # Process the combined text file to remove stop words\n","    with open(output_file_path, 'r', encoding='utf-8') as file:\n","        combined_text = file.read()\n","\n","    processed_text = process_text(combined_text)\n","\n","    # Save the processed text back to the file\n","    save_to_text_file(processed_text, output_file_path)\n","\n","def process_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Remove special characters using regex\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Tokenize the text\n","    words = nltk.word_tokenize(text)\n","\n","    company_stopwords = [\n","        \"download\", \"www\", \"html\", \"http\", \"login\", \"menu\", \"chat\", \"article\", \"disclaimer\", \"facebook\", \"cart\", \"loading\",\n","        \"click\", \"com\", \"htm\", \"https\", \"logout\", \"navbar\", \"message\", \"blog\", \"copyright\", \"twitter\", \"checkout\",\n","        \"processing\", \"submit\", \"org\", \"php\", \"ftp\", \"register\", \"footer\", \"reply\", \"post\", \"terms\", \"instagram\", \"order\",\n","        \"waiting\", \"login\", \"net\", \"asp\", \"mailto\", \"signup\", \"sidebar\", \"comment\", \"news\", \"privacy\", \"linkedin\",\n","        \"invoice\", \"error\", \"logout\", \"gov\", \"jsp\", \"tel\", \"signin\", \"header\", \"post\", \"story\", \"policy\", \"pinterest\",\n","        \"billing\", \"success\", \"register\", \"edu\", \"css\", \"news\", \"signout\", \"banner\", \"thread\", \"update\", \"conditions\",\n","        \"youtube\", \"shipping\", \"failure\", \"sign up\", \"co\", \"js\", \"irc\", \"user\", \"ad\", \"forum\", \"headline\", \"agreement\",\n","        \"vimeo\", \"payment\", \"retry\", \"sign in\", \"io\", \"json\",\"file\", \"account\", \"advertisement\", \"discussion\", \"media\",\n","        \"license\", \"flickr\", \"subscribe\", \"refresh\", \"sign out\", \"uk\", \"xml\", \"profile\", \"promo\", \"like\",\n","        \"video\", \"cookie\", \"reddit\", \"membership\", \"reload\", \"contact\", \"de\", \"pdf\", \"href\", \"admin\", \"button\",\n","        \"share\", \"image\", \"settings\", \"tumblr\", \"account\", \"redirect\", \"about\", \"jp\", \"doc\", \"src\", \"dashboard\",\n","        \"click\", \"follow\", \"photo\", \"preferences\", \"snapchat\", \"profile\", \"navigate\", \"home\", \"fr\", \"docx\", \"ref\",\n","        \"settings\", \"read more\", \"subscribe\", \"gallery\", \"options\", \"tiktok\", \"wishlist\", \"submit\", \"menu\", \"au\",\n","        \"xls\", \"utm_source\", \"search\", \"more info\", \"unsubscribe\", \"slideshow\", \"tools\", \"whatsapp\", \"product\", \"validate\",\n","        \"search\", \"ca\", \"xlsx\", \"utm_medium\", \"results\", \"next\", \"notification\", \"podcast\", \"utilities\", \"telegram\",\n","        \"service\", \"authenticate\", \"next\", \"us\", \"ppt\", \"utm_campaign\", \"help\", \"previous\", \"alert\", \"episode\",\n","        \"resources\", \"messenger\", \"pricing\", \"authorize\", \"previous\", \"in\", \"pptx\", \"utm_term\", \"support\", \"back\",\n","        \"update\", \"stream\", \"links\", \"skype\", \"offer\", \"encrypt\", \"back\", \"cn\", \"txt\", \"utm_content\", \"about\", \"top\",\n","        \"upload\", \"broadcast\", \"map\", \"discord\", \"discount\", \"help\", \"br\", \"zip\", \"param\", \"home\", \"bottom\", \"attachment\",\n","        \"channel\", \"navigation\", \"signal\", \"coupon\", \"support\", \"es\", \"rar\", \"sid\", \"news\", \"skip\", \"link\", \"playlist\",\n","        \"sitemap\", \"medium\", \"gift\", \"terms\", \"tar\", \"id\", \"blog\", \"submit\",\"url\", \"archive\", \"blogspot\", \"buy\",\n","        \"conditions\", \"gz\", \"key\", \"post\", \"reset\", \"address\", \"library\", \"wordpress\", \"sell\", \"privacy\", \"exe\", \"token\",\n","        \"article\", \"cancel\", \"contact\", \"resource\", \"github\", \"rent\", \"policy\", \"dmg\", \"hash\", \"category\", \"edit\",\n","        \"phone\", \"bitbucket\", \"lease\", \"disclaimer\", \"iso\", \"index\", \"tag\", \"delete\", \"email\", \"stackoverflow\", \"booking\",\n","        \"sitemap\", \"bin\", \"page\", \"archive\", \"save\", \"support\", \"quora\", \"reservation\", \"faq\", \"img\", \"sort\", \"year\",\n","        \"print\", \"help\", \"meetup\", \"feedback\", \"filter\", \"month\", \"close\", \"faq\", \"eventbrite\", \"news\", \"author\",\n","        \"collapse\", \"tutorial\", \"blog\", \"faq\", \"dropdown\", \"documentation\", \"post\", \"terms\", \"form\", \"manual\",\n","        \"article\", \"privacy\", \"field\", \"report\", \"read more\", \"policy\", \"checkbox\", \"feedback\", \"follow us\",\n","        \"conditions\", \"radio\", \"survey\", \"share\", \"disclaimer\", \"select\", \"like\", \"sitemap\", \"option\",\n","        \"comment\", \"feedback\", \"input\", \"subscribe\", \"gallery\", \"text area\", \"unsubscribe\", \"media\", \"captcha\",\n","        \"learn more\", \"video\", \"view details\", \"image\", \"all rights reserved\", \"photo\", \"© (copyright)\", \"download\",\n","        \"trademark\", \"upload\", \"update\", \"file\", \"settings\", \"attachment\", \"profile\", \"resource\", \"account\", \"link\",\n","        \"admin\", \"share\", \"dashboard\", \"my account\", \"your account\", \"preferences\", \"notifications\", \"messages\", \"inbox\",\n","        \"outbox\", \"send\", \"receive\", \"cart\", \"checkout\", \"order\", \"payment\", \"invoice\", \"billing\", \"shipping\", \"address\",\n","        \"terms of service\", \"conditions of use\", \"user agreement\", \"cookies\", \"advertisement\", \"sponsor\", \"partnership\",\n","        \"careers\", \"jobs\", \"vacancies\", \"apply now\", \"application\", \"newsletter\", \"updates\", \"events\", \"calendar\",\n","        \"press\", \"release\", \"media\", \"gallery\", \"videos\", \"photos\", \"terms & conditions\", \"privacy & policy\", \"contact us\",\n","        \"back to top\", \"accessibility\", \"languages\", \"select language\", \"international\", \"mobile\", \"desktop\"\n","    ]\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    # Update the stopwords list with company-specific stopwords\n","    stop_words.update(company_stopwords)\n","\n","    # Remove stopwords\n","    words = [word for word in words if word not in stop_words]\n","\n","    # Lemmatize the words\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    return ' '.join(lemmatized_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h24sxoaywihJ","outputId":"bd5b073d-9255-45ec-ea99-560a9df10426"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["Processing hul:  75%|███████▌  | 3/4 [00:07<00:02,  2.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Careers link not found.\n"]},{"output_type":"stream","name":"stderr","text":["Processing hul: 100%|██████████| 4/4 [00:19<00:00,  4.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Processed text saved to hul_scraped_data/processed_text_from_pdfs.txt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["All content extracted and combined into hul_scraped_data/processed_html_content.txt\n","Error scraping https://www.hul.co.in: [Errno 2] No such file or directory: 'hul_scraped_data/hul_careers_scraped_data'\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing icicibank:   0%|          | 0/4 [00:00<?, ?it/s]WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78f7d11e9cc0>: Failed to resolve 'www.icicibank.com%20' ([Errno -2] Name or service not known)\")': /\n","WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78f7d05befe0>: Failed to resolve 'www.icicibank.com%20' ([Errno -2] Name or service not known)\")': /\n","WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78f7d11eb460>: Failed to resolve 'www.icicibank.com%20' ([Errno -2] Name or service not known)\")': /\n","Processing icicibank:  75%|███████▌  | 3/4 [00:00<00:00, 200.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Error fetching https://www.icicibank.com : HTTPSConnectionPool(host='www.icicibank.com%20', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78f7d11e9540>: Failed to resolve 'www.icicibank.com%20' ([Errno -2] Name or service not known)\"))\n","Error fetching page: HTTPSConnectionPool(host='www.icicibank.com%20', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78f7d108b610>: Failed to resolve 'www.icicibank.com%20' ([Errno -2] Name or service not known)\"))\n","Error scraping https://www.icicibank.com : [Errno 2] No such file or directory: 'icicibank_scraped_data/documents'\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing iciciprulife:   0%|          | 0/4 [00:00<?, ?it/s]"]}],"source":["# prompt: create a new column in the same excel called \"Scraping status\". After successful scraping of a company, enter \"Successful\" under this column for that particular company. If scraped fails, enter \"Failed\".\n","# prompt: if the code faces error reading a pdf file, use a different library like 'PyMuPDF' to handle those files.\n","# prompt: after the code ends, ask the user to run for the next 5 urls or to stop\n","# prompt: Create a progress bar for this scraper code to visualize the whole progress per company\n","# prompt: in the progress bar mention the name of company so that we can identify them\n","# prompt: after Updating the range for the next set of URLs, iterate the loop for scraping for next 5 companies\n","\n","import os\n","import shutil\n","import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin, urlparse\n","import hashlib\n","from urllib3.exceptions import InsecureRequestWarning\n","from requests.adapters import HTTPAdapter\n","from requests.packages.urllib3.util.retry import Retry\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from pdfminer.high_level import extract_text\n","from google.colab import drive\n","import pandas as pd\n","import fitz  # PyMuPDF\n","from tqdm import tqdm  # Import tqdm for progress bar\n","\n","\n","drive.mount('/content/drive')\n","\n","def main():\n","    # Read the Excel file\n","    df = pd.read_excel('/content/drive/MyDrive/Company_list.xlsx')\n","\n","    # Create a new column for scraping status\n","    if 'Scraping Status' not in df.columns:\n","        df['Scraping Status'] = ''\n","\n","    start_index = 56\n","    end_index = 60\n","\n","    while True:\n","        # Iterate over the URLs in the current range\n","        for i in range(start_index, end_index):\n","            if i >= len(df):  # Check if we've reached the end of the DataFrame\n","                break  # Exit the inner loop if no more URLs\n","\n","            company_url = df.loc[i, 'Website URL'].strip('\\'\"')\n","            parsed_url = urlparse(company_url)\n","            domain = parsed_url.netloc.split('.')[1] if 'www' in parsed_url.netloc else parsed_url.netloc.split('.')[0]\n","            output_folder = f'{domain}_scraped_data'\n","            common_tabs = ['#main-nav', '.footer', 'header', 'footer']\n","            Careers = f'{domain}_careers_scraped_data'\n","\n","            # Extract company name for progress bar (assuming it's in a column named 'Company Name')\n","            company_name = df.loc[i, 'Company Name'] if 'Company Name' in df.columns else domain\n","\n","            try:\n","                # Step 1: Scrape the website and career sections\n","                with tqdm(total=4, desc=f\"Processing {company_name}\") as pbar:  # Progress bar with company name\n","                    create_folder(output_folder)\n","                    pbar.update(1)\n","                    scrape_website(company_url, output_folder, common_tabs=common_tabs)\n","                    pbar.update(1)\n","                    scrape_careers_section(company_url)\n","                    pbar.update(1)\n","\n","                    # Step 2: Process PDFs from the documents folder\n","                    documents_folder = os.path.join(output_folder, 'documents')\n","                    pdf_output_file = os.path.join(output_folder, 'processed_text_from_pdfs.txt')\n","                    process_pdfs(documents_folder, pdf_output_file)\n","                    pbar.update(1)\n","\n","                # Step 3: Combine HTML content into a single text file\n","                html_folder = os.path.join(output_folder, 'htmls')\n","                combined_html_file = os.path.join(output_folder, 'processed_html_content.txt')\n","                process_html_files_in_directory(html_folder, combined_html_file)\n","\n","                #Step 4: Combine Careers content into a single text file\n","                Careers_folder = os.path.join(output_folder, Careers)\n","                combined_Careers_file = os.path.join(output_folder, 'processed_careers_content.txt')\n","                process_html_files_in_directory(Careers_folder, combined_Careers_file)\n","\n","                # After scraping and processing:\n","                source_folder = f'{domain}_scraped_data'\n","                destination_parent_folder = '/content/drive/MyDrive/Company_data/'\n","\n","                # Construct the full destination path including the source folder name\n","                destination_folder = os.path.join(destination_parent_folder, os.path.basename(source_folder))\n","\n","                # Copy the folder\n","                shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n","                print(f\"Data for {company_url} saved to: {destination_folder}\")\n","\n","                # Mark scraping as successful\n","                df.loc[i, 'Scraping Status'] = 'Successful'\n","\n","            except Exception as e:\n","                print(f\"Error scraping {company_url}: {e}\")\n","                df.loc[i, 'Scraping Status'] = 'Failed'\n","\n","        # Save the updated DataFrame back to Excel\n","        df.to_excel('/content/drive/MyDrive/Company_list.xlsx', index=False)\n","\n","        # Ask the user if they want to continue\n","        choice = input(\"Process next 5 URLs? (y/n): \")\n","        if choice.lower() != 'y':\n","            break\n","\n","        # Update the range for the next set of URLs\n","        start_index = end_index\n","        end_index += 4\n","\n","    print(\"Task completed\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9wVlJi7fIIS"},"outputs":[],"source":["# to prevent disconnect\n","function ClickConnect() {\n","  console.log('Working')\n","  document\n","    .querySelector('#top-toolbar > colab-connect-button')\n","    .shadowRoot.querySelector('#connect')\n","    .click()\n","}\n","intervalTiming = setInterval(ClickConnect, 60000)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}