{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee1fde-1cb7-4cc9-9278-c55098181875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above one works but for few files it throws error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da0125-909f-4d00-9782-8d4e87ac93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38cbfece-80a8-4586-9351-59af88b04ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amitb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amitb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amitb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the folder containing PDFs:  \"C:\\Users\\amitb\\Desktop\\Coding\\Industry 4.0 project\\Full data\\cipla_scraped_data\\documents\"\n",
      "Enter the name of the output text file (e.g., output.txt):  cipla_final_latest.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading 0957c46fe6722d34a20b03919935fe93.pdf: 'PDFObjRef' object is not iterable\n",
      "Error reading 09cdd89e200782c2e5f41fd6cd6e17b2.pdf: Invalid dictionary construct: [/'AntiAlias', /b'fals', /b'e', /'ColorSpace', /'DeviceRGB', /'Coords', [0, 0, 0, 0, 0, 1], /'Domain', [0, 1], /'Extend', [True, True], /'Function', <PDFObjRef:237>, /'ShadingType', 3]\n",
      "Error reading f269f6b3f540991a8974c8db822476d7.pdf: Invalid dictionary construct: [/'I', /b'fal', /b'se', /'K', False, /'S', /'Transparency', /'Type', /'Group']\n",
      "Processed text saved to cipla_final_latest.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Ensure necessary NLTK data packages are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                text = extract_text(file_path)\n",
    "                combined_text += text\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "def process_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters using regex\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    company_stopwords = [\n",
    "        \"download\", \"www\", \"html\", \"http\", \"login\", \"menu\", \"chat\", \"article\", \"disclaimer\", \"facebook\", \"cart\", \"loading\",\n",
    "        \"click\", \"com\", \"htm\", \"https\", \"logout\", \"navbar\", \"message\", \"blog\", \"copyright\", \"twitter\", \"checkout\",\n",
    "        \"processing\", \"submit\", \"org\", \"php\", \"ftp\", \"register\", \"footer\", \"reply\", \"post\", \"terms\", \"instagram\", \"order\",\n",
    "        \"waiting\", \"login\", \"net\", \"asp\", \"mailto\", \"signup\", \"sidebar\", \"comment\", \"news\", \"privacy\", \"linkedin\",\n",
    "        \"invoice\", \"error\", \"logout\", \"gov\", \"jsp\", \"tel\", \"signin\", \"header\", \"post\", \"story\", \"policy\", \"pinterest\",\n",
    "        \"billing\", \"success\", \"register\", \"edu\", \"css\", \"news\", \"signout\", \"banner\", \"thread\", \"update\", \"conditions\",\n",
    "        \"youtube\", \"shipping\", \"failure\", \"sign up\", \"co\", \"js\", \"irc\", \"user\", \"ad\", \"forum\", \"headline\", \"agreement\",\n",
    "        \"vimeo\", \"payment\", \"retry\", \"sign in\", \"io\", \"json\",\"file\", \"account\", \"advertisement\", \"discussion\", \"media\",\n",
    "        \"license\", \"flickr\", \"subscribe\", \"refresh\", \"sign out\", \"uk\", \"xml\", \"profile\", \"promo\", \"like\",\n",
    "        \"video\", \"cookie\", \"reddit\", \"membership\", \"reload\", \"contact\", \"de\", \"pdf\", \"href\", \"admin\", \"button\",\n",
    "        \"share\", \"image\", \"settings\", \"tumblr\", \"account\", \"redirect\", \"about\", \"jp\", \"doc\", \"src\", \"dashboard\",\n",
    "        \"click\", \"follow\", \"photo\", \"preferences\", \"snapchat\", \"profile\", \"navigate\", \"home\", \"fr\", \"docx\", \"ref\",\n",
    "        \"settings\", \"read more\", \"subscribe\", \"gallery\", \"options\", \"tiktok\", \"wishlist\", \"submit\", \"menu\", \"au\",\n",
    "        \"xls\", \"utm_source\", \"search\", \"more info\", \"unsubscribe\", \"slideshow\", \"tools\", \"whatsapp\", \"product\", \"validate\",\n",
    "        \"search\", \"ca\", \"xlsx\", \"utm_medium\", \"results\", \"next\", \"notification\", \"podcast\", \"utilities\", \"telegram\",\n",
    "        \"service\", \"authenticate\", \"next\", \"us\", \"ppt\", \"utm_campaign\", \"help\", \"previous\", \"alert\", \"episode\",\n",
    "        \"resources\", \"messenger\", \"pricing\", \"authorize\", \"previous\", \"in\", \"pptx\", \"utm_term\", \"support\", \"back\",\n",
    "        \"update\", \"stream\", \"links\", \"skype\", \"offer\", \"encrypt\", \"back\", \"cn\", \"txt\", \"utm_content\", \"about\", \"top\",\n",
    "        \"upload\", \"broadcast\", \"map\", \"discord\", \"discount\", \"help\", \"br\", \"zip\", \"param\", \"home\", \"bottom\", \"attachment\",\n",
    "        \"channel\", \"navigation\", \"signal\", \"coupon\", \"support\", \"es\", \"rar\", \"sid\", \"news\", \"skip\", \"link\", \"playlist\",\n",
    "        \"sitemap\", \"medium\", \"gift\", \"terms\", \"tar\", \"id\", \"blog\", \"submit\",\"url\", \"archive\", \"blogspot\", \"buy\",\n",
    "        \"conditions\", \"gz\", \"key\", \"post\", \"reset\", \"address\", \"library\", \"wordpress\", \"sell\", \"privacy\", \"exe\", \"token\",\n",
    "        \"article\", \"cancel\", \"contact\", \"resource\", \"github\", \"rent\", \"policy\", \"dmg\", \"hash\", \"category\", \"edit\",\n",
    "        \"phone\", \"bitbucket\", \"lease\", \"disclaimer\", \"iso\", \"index\", \"tag\", \"delete\", \"email\", \"stackoverflow\", \"booking\",\n",
    "        \"sitemap\", \"bin\", \"page\", \"archive\", \"save\", \"support\", \"quora\", \"reservation\", \"faq\", \"img\", \"sort\", \"year\",\n",
    "        \"print\", \"help\", \"meetup\", \"feedback\", \"filter\", \"month\", \"close\", \"faq\", \"eventbrite\", \"news\", \"author\",\n",
    "        \"collapse\", \"tutorial\", \"blog\", \"faq\", \"dropdown\", \"documentation\", \"post\", \"terms\", \"form\", \"manual\",\n",
    "        \"article\", \"privacy\", \"field\", \"report\", \"read more\", \"policy\", \"checkbox\", \"feedback\", \"follow us\",\n",
    "        \"conditions\", \"radio\", \"survey\", \"share\", \"disclaimer\", \"select\", \"like\", \"sitemap\", \"option\",\n",
    "        \"comment\", \"feedback\", \"input\", \"subscribe\", \"gallery\", \"text area\", \"unsubscribe\", \"media\", \"captcha\",\n",
    "        \"learn more\", \"video\", \"view details\", \"image\", \"all rights reserved\", \"photo\", \"Â© (copyright)\", \"download\",\n",
    "        \"trademark\", \"upload\", \"update\", \"file\", \"settings\", \"attachment\", \"profile\", \"resource\", \"account\", \"link\",\n",
    "        \"admin\", \"share\", \"dashboard\", \"my account\", \"your account\", \"preferences\", \"notifications\", \"messages\", \"inbox\",\n",
    "        \"outbox\", \"send\", \"receive\", \"cart\", \"checkout\", \"order\", \"payment\", \"invoice\", \"billing\", \"shipping\", \"address\",\n",
    "        \"terms of service\", \"conditions of use\", \"user agreement\", \"cookies\", \"advertisement\", \"sponsor\", \"partnership\",\n",
    "        \"careers\", \"jobs\", \"vacancies\", \"apply now\", \"application\", \"newsletter\", \"updates\", \"events\", \"calendar\",\n",
    "        \"press\", \"release\", \"media\", \"gallery\", \"videos\", \"photos\", \"terms & conditions\", \"privacy & policy\", \"contact us\",\n",
    "        \"back to top\", \"accessibility\", \"languages\", \"select language\", \"international\", \"mobile\", \"desktop\"\n",
    "    ]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Update the stopwords list with company-specific stopwords\n",
    "    stop_words.update(company_stopwords)\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def main():\n",
    "    input_folder = input(\"Enter the path to the folder containing PDFs: \").strip('\\'\"')\n",
    "    output_file = input(\"Enter the name of the output text file (e.g., output.txt): \").strip('\\'\"')\n",
    "    \n",
    "    if not os.path.isdir(input_folder):\n",
    "        print(f\"The directory {input_folder} does not exist. Please check the path and try again.\")\n",
    "        return\n",
    "    \n",
    "    # Extract text from PDFs\n",
    "    combined_text = extract_text_from_pdfs(input_folder)\n",
    "    \n",
    "    # Process the text\n",
    "    processed_text = process_text(combined_text)\n",
    "    \n",
    "    # Write the processed text to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_text)\n",
    "    \n",
    "    print(f\"Processed text saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a96585-623d-49bc-962a-ddf7f80e3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this document parser works\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
