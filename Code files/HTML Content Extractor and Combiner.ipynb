{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91dfdf1e",
   "metadata": {},
   "source": [
    "Instructions\n",
    "\n",
    "    Set the Directory Path: Update the directory_path variable to the path of the directory containing your HTML files.\n",
    "    Set the Output File Path: Update the output_file_path variable to the desired path for the combined output text file.\n",
    "    Run the Script: Execute the script in your Python environment. It will process all HTML files in the specified directory and combine the extracted content into a single text file.\n",
    "\n",
    "This approach ensures that all HTML files in the specified directory are processed, and their main content is combined into a single output file, with no changes to the header, footer, or other irrelevant parts of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fafeb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitb\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All content extracted and combined into C:\\Users\\amitb\\Desktop\\Coding\\Industry 4.0 project\\wipro_careers_scraped_data\\combined_html_content.txt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def extract_main_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove header, footer, nav, script, style, and noscript elements\n",
    "    for element in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):\n",
    "        element.decompose()\n",
    "\n",
    "    # Find the main content area\n",
    "    main_content = \"\"\n",
    "    article = soup.find('article')\n",
    "    if article:\n",
    "        main_content = article.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        main = soup.find('main')\n",
    "        if main:\n",
    "            main_content = main.get_text(separator='\\n', strip=True)\n",
    "        else:\n",
    "            large_divs = soup.find_all('div')\n",
    "            for div in large_divs:\n",
    "                if len(div.get_text(strip=True)) > 200:  # Threshold length to identify large content blocks\n",
    "                    main_content += div.get_text(separator='\\n', strip=True) + '\\n'\n",
    "            if not main_content:  # Fallback if no large divs found\n",
    "                body = soup.find('body')\n",
    "                if body:\n",
    "                    main_content = body.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    return main_content\n",
    "\n",
    "def save_to_text_file(content, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def process_html_files_in_directory(directory_path, output_file_path):\n",
    "    combined_content = \"\"\n",
    "    \n",
    "    # Iterate through each file in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.html'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Extract the main content\n",
    "            main_content = extract_main_content(html_content)\n",
    "            combined_content += main_content + \"\\n\\n\"\n",
    "\n",
    "    # Save the combined content to a single text file\n",
    "    save_to_text_file(combined_content, output_file_path)\n",
    "\n",
    "# Directory containing HTML files\n",
    "directory_path = r'C:\\Users\\amitb\\Desktop\\Coding\\Industry 4.0 project\\wipro_careers_scraped_data\\links'\n",
    "\n",
    "# Output file path\n",
    "output_file_path = r'C:\\Users\\amitb\\Desktop\\Coding\\Industry 4.0 project\\wipro_careers_scraped_data\\combined_html_content.txt'\n",
    "\n",
    "# Process HTML files and combine content\n",
    "process_html_files_in_directory(directory_path, output_file_path)\n",
    "\n",
    "print(f'All content extracted and combined into {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27554af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
