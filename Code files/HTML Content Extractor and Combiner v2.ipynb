{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65181d5-baa0-4259-bc94-33be4e3843d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amitb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amitb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amitb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All content extracted, combined, and processed into C:\\\\Users\\\\amitb\\\\Desktop\\\\Coding\\\\Industry 4.0 project\\\\v2_tcs_html_combined.txt\n",
      "All content extracted, combined, and processed into C:\\\\Users\\\\amitb\\\\Desktop\\\\Coding\\\\Industry 4.0 project\\\\v2_tcs_html_combined.txt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def extract_main_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove header, footer, nav, script, style, and noscript elements\n",
    "    for element in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):\n",
    "        element.decompose()\n",
    "\n",
    "    # Find the main content area\n",
    "    main_content = \"\"\n",
    "    article = soup.find('article')\n",
    "    if article:\n",
    "        main_content = article.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        main = soup.find('main')\n",
    "        if main:\n",
    "            main_content = main.get_text(separator='\\n', strip=True)\n",
    "        else:\n",
    "            large_divs = soup.find_all('div')\n",
    "            for div in large_divs:\n",
    "                if len(div.get_text(strip=True)) > 200:  # Threshold length to identify large content blocks\n",
    "                    main_content += div.get_text(separator='\\n', strip=True) + '\\n'\n",
    "            if not main_content:  # Fallback if no large divs found\n",
    "                body = soup.find('body')\n",
    "                if body:\n",
    "                    main_content = body.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    return main_content\n",
    "\n",
    "def save_to_text_file(content, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def process_html_files_in_directory(directory_path, output_file_path):\n",
    "    combined_content = \"\"\n",
    "    \n",
    "    # Iterate through each file in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.html'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Extract the main content\n",
    "            main_content = extract_main_content(html_content)\n",
    "            combined_content += main_content + \"\\n\\n\"\n",
    "\n",
    "    # Save the combined content to a single text file\n",
    "    save_to_text_file(combined_content, output_file_path)\n",
    "\n",
    "    # Process the combined text file to remove stop words\n",
    "    with open(output_file_path, 'r', encoding='utf-8') as file:\n",
    "        combined_text = file.read()\n",
    "\n",
    "    processed_text = process_text(combined_text)\n",
    "\n",
    "    # Save the processed text back to the file\n",
    "    save_to_text_file(processed_text, output_file_path)\n",
    "\n",
    "    print(f'All content extracted, combined, and processed into {output_file_path}')\n",
    "\n",
    "def process_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters using regex\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    company_stopwords = [\n",
    "        \"download\", \"www\", \"html\", \"http\", \"login\", \"menu\", \"chat\", \"article\", \"disclaimer\", \"facebook\", \"cart\", \"loading\",\n",
    "        \"click\", \"com\", \"htm\", \"https\", \"logout\", \"navbar\", \"message\", \"blog\", \"copyright\", \"twitter\", \"checkout\",\n",
    "        \"processing\", \"submit\", \"org\", \"php\", \"ftp\", \"register\", \"footer\", \"reply\", \"post\", \"terms\", \"instagram\", \"order\",\n",
    "        \"waiting\", \"login\", \"net\", \"asp\", \"mailto\", \"signup\", \"sidebar\", \"comment\", \"news\", \"privacy\", \"linkedin\",\n",
    "        \"invoice\", \"error\", \"logout\", \"gov\", \"jsp\", \"tel\", \"signin\", \"header\", \"post\", \"story\", \"policy\", \"pinterest\",\n",
    "        \"billing\", \"success\", \"register\", \"edu\", \"css\", \"news\", \"signout\", \"banner\", \"thread\", \"update\", \"conditions\",\n",
    "        \"youtube\", \"shipping\", \"failure\", \"sign up\", \"co\", \"js\", \"irc\", \"user\", \"ad\", \"forum\", \"headline\", \"agreement\",\n",
    "        \"vimeo\", \"payment\", \"retry\", \"sign in\", \"io\", \"json\",\"file\", \"account\", \"advertisement\", \"discussion\", \"media\",\n",
    "        \"license\", \"flickr\", \"subscribe\", \"refresh\", \"sign out\", \"uk\", \"xml\", \"profile\", \"promo\", \"like\",\n",
    "        \"video\", \"cookie\", \"reddit\", \"membership\", \"reload\", \"contact\", \"de\", \"pdf\", \"href\", \"admin\", \"button\",\n",
    "        \"share\", \"image\", \"settings\", \"tumblr\", \"account\", \"redirect\", \"about\", \"jp\", \"doc\", \"src\", \"dashboard\",\n",
    "        \"click\", \"follow\", \"photo\", \"preferences\", \"snapchat\", \"profile\", \"navigate\", \"home\", \"fr\", \"docx\", \"ref\",\n",
    "        \"settings\", \"read more\", \"subscribe\", \"gallery\", \"options\", \"tiktok\", \"wishlist\", \"submit\", \"menu\", \"au\",\n",
    "        \"xls\", \"utm_source\", \"search\", \"more info\", \"unsubscribe\", \"slideshow\", \"tools\", \"whatsapp\", \"product\", \"validate\",\n",
    "        \"search\", \"ca\", \"xlsx\", \"utm_medium\", \"results\", \"next\", \"notification\", \"podcast\", \"utilities\", \"telegram\",\n",
    "        \"service\", \"authenticate\", \"next\", \"us\", \"ppt\", \"utm_campaign\", \"help\", \"previous\", \"alert\", \"episode\",\n",
    "        \"resources\", \"messenger\", \"pricing\", \"authorize\", \"previous\", \"in\", \"pptx\", \"utm_term\", \"support\", \"back\",\n",
    "        \"update\", \"stream\", \"links\", \"skype\", \"offer\", \"encrypt\", \"back\", \"cn\", \"txt\", \"utm_content\", \"about\", \"top\",\n",
    "        \"upload\", \"broadcast\", \"map\", \"discord\", \"discount\", \"help\", \"br\", \"zip\", \"param\", \"home\", \"bottom\", \"attachment\",\n",
    "        \"channel\", \"navigation\", \"signal\", \"coupon\", \"support\", \"es\", \"rar\", \"sid\", \"news\", \"skip\", \"link\", \"playlist\",\n",
    "        \"sitemap\", \"medium\", \"gift\", \"terms\", \"tar\", \"id\", \"blog\", \"submit\",\"url\", \"archive\", \"blogspot\", \"buy\",\n",
    "        \"conditions\", \"gz\", \"key\", \"post\", \"reset\", \"address\", \"library\", \"wordpress\", \"sell\", \"privacy\", \"exe\", \"token\",\n",
    "        \"article\", \"cancel\", \"contact\", \"resource\", \"github\", \"rent\", \"policy\", \"dmg\", \"hash\", \"category\", \"edit\",\n",
    "        \"phone\", \"bitbucket\", \"lease\", \"disclaimer\", \"iso\", \"index\", \"tag\", \"delete\", \"email\", \"stackoverflow\", \"booking\",\n",
    "        \"sitemap\", \"bin\", \"page\", \"archive\", \"save\", \"support\", \"quora\", \"reservation\", \"faq\", \"img\", \"sort\", \"year\",\n",
    "        \"print\", \"help\", \"meetup\", \"feedback\", \"filter\", \"month\", \"close\", \"faq\", \"eventbrite\", \"news\", \"author\",\n",
    "        \"collapse\", \"tutorial\", \"blog\", \"faq\", \"dropdown\", \"documentation\", \"post\", \"terms\", \"form\", \"manual\",\n",
    "        \"article\", \"privacy\", \"field\", \"report\", \"read more\", \"policy\", \"checkbox\", \"feedback\", \"follow us\",\n",
    "        \"conditions\", \"radio\", \"survey\", \"share\", \"disclaimer\", \"select\", \"like\", \"sitemap\", \"option\",\n",
    "        \"comment\", \"feedback\", \"input\", \"subscribe\", \"gallery\", \"text area\", \"unsubscribe\", \"media\", \"captcha\",\n",
    "        \"learn more\", \"video\", \"view details\", \"image\", \"all rights reserved\", \"photo\", \"Â© (copyright)\", \"download\",\n",
    "        \"trademark\", \"upload\", \"update\", \"file\", \"settings\", \"attachment\", \"profile\", \"resource\", \"account\", \"link\",\n",
    "        \"admin\", \"share\", \"dashboard\", \"my account\", \"your account\", \"preferences\", \"notifications\", \"messages\", \"inbox\",\n",
    "        \"outbox\", \"send\", \"receive\", \"cart\", \"checkout\", \"order\", \"payment\", \"invoice\", \"billing\", \"shipping\", \"address\",\n",
    "        \"terms of service\", \"conditions of use\", \"user agreement\", \"cookies\", \"advertisement\", \"sponsor\", \"partnership\",\n",
    "        \"careers\", \"jobs\", \"vacancies\", \"apply now\", \"application\", \"newsletter\", \"updates\", \"events\", \"calendar\",\n",
    "        \"press\", \"release\", \"media\", \"gallery\", \"videos\", \"photos\", \"terms & conditions\", \"privacy & policy\", \"contact us\",\n",
    "        \"back to top\", \"accessibility\", \"languages\", \"select language\", \"international\", \"mobile\", \"desktop\"\n",
    "    ]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Update the stopwords list with company-specific stopwords\n",
    "    stop_words.update(company_stopwords)\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Directory containing HTML files\n",
    "directory_path = r\"C:\\Users\\amitb\\Desktop\\Coding\\Industry 4.0 project\\Full data\\Latest_tcs_scraped_data\\htmls\"\n",
    "\n",
    "# Output file path\n",
    "output_file_path = r'C:\\\\Users\\\\amitb\\\\Desktop\\\\Coding\\\\Industry 4.0 project\\\\v2_tcs_html_combined.txt'\n",
    "\n",
    "# Process HTML files and combine content\n",
    "process_html_files_in_directory(directory_path, output_file_path)\n",
    "\n",
    "print(f'All content extracted, combined, and processed into {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec2a3c-601c-451b-9f67-11d0f3bb9a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
