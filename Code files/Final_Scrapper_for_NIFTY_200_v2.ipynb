{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaZ5qEWAFB4d",
        "outputId": "a3c3c5ed-e309-4f9e-d51f-ea354c3bbd75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Collecting requests-toolbelt\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from requests-toolbelt) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (2024.7.4)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: requests-toolbelt\n",
            "Successfully installed requests-toolbelt-1.0.0\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install urllib3\n",
        "!pip install requests-toolbelt\n",
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nofSlwPbEuPC",
        "outputId": "5a97310f-008d-4f64-86fb-d72064498ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import hashlib\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Ensure necessary NLTK data packages are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BElOoWpMEl7O"
      },
      "outputs": [],
      "source": [
        "\n",
        "###---------------------------------------------------------------scraper-------------------------------------------------\n",
        "\n",
        "# Suppress SSL certificate warnings (use with caution)\n",
        "requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)\n",
        "\n",
        "def create_folder(folder_path):\n",
        "    \"\"\"Creates a folder if it doesn't exist.\"\"\"\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "def get_unique_filename(url):\n",
        "    \"\"\"Generates a unique filename based on URL hash.\"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    file_name = hashlib.md5(url.encode()).hexdigest()\n",
        "    return file_name + \".html\"\n",
        "\n",
        "def get_document_filename(url):\n",
        "    \"\"\"Extracts the filename from the URL for documents.\"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    return os.path.basename(parsed_url.path)\n",
        "\n",
        "def save_content(content, folder_path, filename):\n",
        "    \"\"\"Saves content (text or binary) to a file.\"\"\"\n",
        "    with open(os.path.join(folder_path, filename), 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def scrape_website(url, folder_path, visited=set(), depth=0, max_depth=3, common_tabs=None):\n",
        "    \"\"\"Recursively scrapes a website up to max_depth.\"\"\"\n",
        "    if depth > max_depth or url in visited:\n",
        "        return\n",
        "\n",
        "    visited.add(url)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, verify=False)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        if common_tabs:\n",
        "            for tab in common_tabs:\n",
        "                for elem in soup.select(tab):\n",
        "                    elem.decompose()\n",
        "\n",
        "        # Save HTML content excluding header, footer, and common tabs\n",
        "        html_folder = os.path.join(folder_path, 'htmls')\n",
        "        create_folder(html_folder)\n",
        "        filtered_content = soup.prettify('utf-8')\n",
        "        save_content(filtered_content, html_folder, get_unique_filename(url))\n",
        "\n",
        "        # Save documents\n",
        "        doc_folder = os.path.join(folder_path, 'documents')\n",
        "        create_folder(doc_folder)\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link.get('href')\n",
        "            full_url = urljoin(url, href)\n",
        "            if urlparse(full_url).netloc == urlparse(url).netloc:\n",
        "                if full_url.endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx')):\n",
        "                    save_content(requests.get(full_url, verify=False).content, doc_folder, get_document_filename(full_url))\n",
        "                else:\n",
        "                    scrape_website(full_url, folder_path, visited, depth + 1, max_depth, common_tabs)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "\n",
        "def safe_filename(text):\n",
        "    # Replace characters not allowed in Windows filenames\n",
        "    return ''.join(c for c in text if c.isalnum() or c in [' ', '_', '-', '.'])\n",
        "\n",
        "def scrape_careers_section(company_url):\n",
        "    # Parse the company URL\n",
        "    parsed_url = urlparse(company_url)\n",
        "    company_name = parsed_url.netloc.replace('www.', '').replace('.com', '').replace('.org', '')  # Extract company name\n",
        "\n",
        "    # Create main directory for scraped data\n",
        "    main_dir = f\"{company_name}_scraped_data\"\n",
        "    if not os.path.exists(main_dir):\n",
        "        os.makedirs(main_dir)\n",
        "\n",
        "    careers_dir = os.path.join(main_dir, f\"{company_name}_careers_scraped_data\")\n",
        "    if not os.path.exists(careers_dir):\n",
        "        os.makedirs(careers_dir)\n",
        "\n",
        "    # Retry mechanism setup\n",
        "    session = requests.Session()\n",
        "    retry_strategy = Retry(\n",
        "        total=3,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session.mount(\"https://\", adapter)\n",
        "    session.mount(\"http://\", adapter)\n",
        "\n",
        "    # Fetch HTML content\n",
        "    try:\n",
        "        response = session.get(company_url, timeout=30, allow_redirects=False)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find link to Careers section\n",
        "            careers_link = None\n",
        "            header_careers_link = soup.find('a', string='Careers')\n",
        "            footer_careers_link = soup.find('a', string='Careers')  # Adjust if different in actual site structure\n",
        "\n",
        "            if header_careers_link:\n",
        "                careers_link = header_careers_link.get('href')\n",
        "            elif footer_careers_link:\n",
        "                careers_link = footer_careers_link.get('href')\n",
        "\n",
        "            if careers_link:\n",
        "                # Convert relative URL to absolute URL\n",
        "                careers_url = urljoin(company_url, careers_link)\n",
        "                careers_response = session.get(careers_url, timeout=30, allow_redirects=False)\n",
        "                if careers_response.status_code == 200:\n",
        "                    careers_soup = BeautifulSoup(careers_response.content, 'html.parser')\n",
        "\n",
        "                    # Example: Find all links in the 'Careers' section\n",
        "                    links = careers_soup.find_all('a', href=True)\n",
        "                    for link in links:\n",
        "                        link_url = urljoin(careers_url, link['href'])  # Convert relative link to absolute\n",
        "                        # Check if link is HTTP/HTTPS\n",
        "                        if link_url.startswith('http://') or link_url.startswith('https://'):\n",
        "                            # Save each link as an HTML file\n",
        "                            filename = safe_filename(link_url.split('/')[-1].replace('.html', '')) + '.html'\n",
        "                            with open(os.path.join(careers_dir, filename), 'w', encoding='utf-8') as f:\n",
        "                                f.write(session.get(link_url, timeout=30, allow_redirects=False).text)\n",
        "\n",
        "                    # Example: Find all downloadable documents (PDFs)\n",
        "                    documents = careers_soup.find_all('a', {'class': 'document'}, href=True)\n",
        "                    for doc in documents:\n",
        "                        doc_url = urljoin(careers_url, doc['href'])  # Convert relative link to absolute\n",
        "                        # Check if link is HTTP/HTTPS\n",
        "                        if doc_url.startswith('http://') or doc_url.startswith('https://'):\n",
        "                            # Download and save documents\n",
        "                            doc_filename = safe_filename(doc_url.split('/')[-1])\n",
        "                            with open(os.path.join(careers_dir, doc_filename), 'wb') as f:\n",
        "                                f.write(session.get(doc_url, timeout=30, allow_redirects=False).content)\n",
        "                else:\n",
        "                    print(f\"Failed to retrieve Careers page: {careers_response.status_code}\")\n",
        "\n",
        "            else:\n",
        "                print(\"Careers link not found.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to retrieve page: {response.status_code}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching page: {e}\")\n",
        "\n",
        "#---------------------------------------------------------document_combiner-----------------------------------------------------------\n",
        "\n",
        "def extract_text_from_pdfs(folder_path):\n",
        "    combined_text = \"\"\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                text = extract_text(file_path)\n",
        "                combined_text += text\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filename}: {e}\")\n",
        "\n",
        "    return combined_text\n",
        "\n",
        "def process_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters using regex\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    company_stopwords = [\n",
        "        \"download\", \"www\", \"html\", \"http\", \"login\", \"menu\", \"chat\", \"article\", \"disclaimer\", \"facebook\", \"cart\", \"loading\",\n",
        "        \"click\", \"com\", \"htm\", \"https\", \"logout\", \"navbar\", \"message\", \"blog\", \"copyright\", \"twitter\", \"checkout\",\n",
        "        \"processing\", \"submit\", \"org\", \"php\", \"ftp\", \"register\", \"footer\", \"reply\", \"post\", \"terms\", \"instagram\", \"order\",\n",
        "        \"waiting\", \"login\", \"net\", \"asp\", \"mailto\", \"signup\", \"sidebar\", \"comment\", \"news\", \"privacy\", \"linkedin\",\n",
        "        \"invoice\", \"error\", \"logout\", \"gov\", \"jsp\", \"tel\", \"signin\", \"header\", \"post\", \"story\", \"policy\", \"pinterest\",\n",
        "        \"billing\", \"success\", \"register\", \"edu\", \"css\", \"news\", \"signout\", \"banner\", \"thread\", \"update\", \"conditions\",\n",
        "        \"youtube\", \"shipping\", \"failure\", \"sign up\", \"co\", \"js\", \"irc\", \"user\", \"ad\", \"forum\", \"headline\", \"agreement\",\n",
        "        \"vimeo\", \"payment\", \"retry\", \"sign in\", \"io\", \"json\",\"file\", \"account\", \"advertisement\", \"discussion\", \"media\",\n",
        "        \"license\", \"flickr\", \"subscribe\", \"refresh\", \"sign out\", \"uk\", \"xml\", \"profile\", \"promo\", \"like\",\n",
        "        \"video\", \"cookie\", \"reddit\", \"membership\", \"reload\", \"contact\", \"de\", \"pdf\", \"href\", \"admin\", \"button\",\n",
        "        \"share\", \"image\", \"settings\", \"tumblr\", \"account\", \"redirect\", \"about\", \"jp\", \"doc\", \"src\", \"dashboard\",\n",
        "        \"click\", \"follow\", \"photo\", \"preferences\", \"snapchat\", \"profile\", \"navigate\", \"home\", \"fr\", \"docx\", \"ref\",\n",
        "        \"settings\", \"read more\", \"subscribe\", \"gallery\", \"options\", \"tiktok\", \"wishlist\", \"submit\", \"menu\", \"au\",\n",
        "        \"xls\", \"utm_source\", \"search\", \"more info\", \"unsubscribe\", \"slideshow\", \"tools\", \"whatsapp\", \"product\", \"validate\",\n",
        "        \"search\", \"ca\", \"xlsx\", \"utm_medium\", \"results\", \"next\", \"notification\", \"podcast\", \"utilities\", \"telegram\",\n",
        "        \"service\", \"authenticate\", \"next\", \"us\", \"ppt\", \"utm_campaign\", \"help\", \"previous\", \"alert\", \"episode\",\n",
        "        \"resources\", \"messenger\", \"pricing\", \"authorize\", \"previous\", \"in\", \"pptx\", \"utm_term\", \"support\", \"back\",\n",
        "        \"update\", \"stream\", \"links\", \"skype\", \"offer\", \"encrypt\", \"back\", \"cn\", \"txt\", \"utm_content\", \"about\", \"top\",\n",
        "        \"upload\", \"broadcast\", \"map\", \"discord\", \"discount\", \"help\", \"br\", \"zip\", \"param\", \"home\", \"bottom\", \"attachment\",\n",
        "        \"channel\", \"navigation\", \"signal\", \"coupon\", \"support\", \"es\", \"rar\", \"sid\", \"news\", \"skip\", \"link\", \"playlist\",\n",
        "        \"sitemap\", \"medium\", \"gift\", \"terms\", \"tar\", \"id\", \"blog\", \"submit\",\"url\", \"archive\", \"blogspot\", \"buy\",\n",
        "        \"conditions\", \"gz\", \"key\", \"post\", \"reset\", \"address\", \"library\", \"wordpress\", \"sell\", \"privacy\", \"exe\", \"token\",\n",
        "        \"article\", \"cancel\", \"contact\", \"resource\", \"github\", \"rent\", \"policy\", \"dmg\", \"hash\", \"category\", \"edit\",\n",
        "        \"phone\", \"bitbucket\", \"lease\", \"disclaimer\", \"iso\", \"index\", \"tag\", \"delete\", \"email\", \"stackoverflow\", \"booking\",\n",
        "        \"sitemap\", \"bin\", \"page\", \"archive\", \"save\", \"support\", \"quora\", \"reservation\", \"faq\", \"img\", \"sort\", \"year\",\n",
        "        \"print\", \"help\", \"meetup\", \"feedback\", \"filter\", \"month\", \"close\", \"faq\", \"eventbrite\", \"news\", \"author\",\n",
        "        \"collapse\", \"tutorial\", \"blog\", \"faq\", \"dropdown\", \"documentation\", \"post\", \"terms\", \"form\", \"manual\",\n",
        "        \"article\", \"privacy\", \"field\", \"report\", \"read more\", \"policy\", \"checkbox\", \"feedback\", \"follow us\",\n",
        "        \"conditions\", \"radio\", \"survey\", \"share\", \"disclaimer\", \"select\", \"like\", \"sitemap\", \"option\",\n",
        "        \"comment\", \"feedback\", \"input\", \"subscribe\", \"gallery\", \"text area\", \"unsubscribe\", \"media\", \"captcha\",\n",
        "        \"learn more\", \"video\", \"view details\", \"image\", \"all rights reserved\", \"photo\", \"© (copyright)\", \"download\",\n",
        "        \"trademark\", \"upload\", \"update\", \"file\", \"settings\", \"attachment\", \"profile\", \"resource\", \"account\", \"link\",\n",
        "        \"admin\", \"share\", \"dashboard\", \"my account\", \"your account\", \"preferences\", \"notifications\", \"messages\", \"inbox\",\n",
        "        \"outbox\", \"send\", \"receive\", \"cart\", \"checkout\", \"order\", \"payment\", \"invoice\", \"billing\", \"shipping\", \"address\",\n",
        "        \"terms of service\", \"conditions of use\", \"user agreement\", \"cookies\", \"advertisement\", \"sponsor\", \"partnership\",\n",
        "        \"careers\", \"jobs\", \"vacancies\", \"apply now\", \"application\", \"newsletter\", \"updates\", \"events\", \"calendar\",\n",
        "        \"press\", \"release\", \"media\", \"gallery\", \"videos\", \"photos\", \"terms & conditions\", \"privacy & policy\", \"contact us\",\n",
        "        \"back to top\", \"accessibility\", \"languages\", \"select language\", \"international\", \"mobile\", \"desktop\"\n",
        "    ]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Update the stopwords list with company-specific stopwords\n",
        "    stop_words.update(company_stopwords)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "def process_pdfs(documents_folder, output_file):\n",
        "    # Extract text from PDFs\n",
        "    combined_text = extract_text_from_pdfs(documents_folder)\n",
        "\n",
        "    # Process the text\n",
        "    processed_text = process_text(combined_text)\n",
        "\n",
        "    # Write the processed text to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(processed_text)\n",
        "\n",
        "    print(f\"Processed text saved to {output_file}\")\n",
        "\n",
        "##---------------------------------------------------------------html_combiner----------------------------------------------------------\n",
        "\n",
        "def extract_main_content(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Remove header, footer, nav, script, style, and noscript elements\n",
        "    for element in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):\n",
        "        element.decompose()\n",
        "\n",
        "    # Find the main content area\n",
        "    main_content = \"\"\n",
        "    article = soup.find('article')\n",
        "    if article:\n",
        "        main_content = article.get_text(separator='\\n', strip=True)\n",
        "    else:\n",
        "        main = soup.find('main')\n",
        "        if main:\n",
        "            main_content = main.get_text(separator='\\n', strip=True)\n",
        "        else:\n",
        "            large_divs = soup.find_all('div')\n",
        "            for div in large_divs:\n",
        "                if len(div.get_text(strip=True)) > 200:  # Threshold length to identify large content blocks\n",
        "                    main_content += div.get_text(separator='\\n', strip=True) + '\\n'\n",
        "            if not main_content:  # Fallback if no large divs found\n",
        "                body = soup.find('body')\n",
        "                if body:\n",
        "                    main_content = body.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    return main_content\n",
        "\n",
        "def save_to_text_file(content, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "def process_html_files_in_directory(directory_path, output_file_path):\n",
        "    combined_content = \"\"\n",
        "\n",
        "    # Iterate through each file in the directory\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith('.html'):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                html_content = file.read()\n",
        "\n",
        "            # Extract the main content\n",
        "            main_content = extract_main_content(html_content)\n",
        "            combined_content += main_content + \"\\n\\n\"\n",
        "\n",
        "    # Save the combined content to a single text file\n",
        "    save_to_text_file(combined_content, output_file_path)\n",
        "    print(f'All content extracted and combined into {output_file_path}')\n",
        "\n",
        "    # Process the combined text file to remove stop words\n",
        "    with open(output_file_path, 'r', encoding='utf-8') as file:\n",
        "        combined_text = file.read()\n",
        "\n",
        "    processed_text = process_text(combined_text)\n",
        "\n",
        "    # Save the processed text back to the file\n",
        "    save_to_text_file(processed_text, output_file_path)\n",
        "\n",
        "def process_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters using regex\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    company_stopwords = [\n",
        "        \"download\", \"www\", \"html\", \"http\", \"login\", \"menu\", \"chat\", \"article\", \"disclaimer\", \"facebook\", \"cart\", \"loading\",\n",
        "        \"click\", \"com\", \"htm\", \"https\", \"logout\", \"navbar\", \"message\", \"blog\", \"copyright\", \"twitter\", \"checkout\",\n",
        "        \"processing\", \"submit\", \"org\", \"php\", \"ftp\", \"register\", \"footer\", \"reply\", \"post\", \"terms\", \"instagram\", \"order\",\n",
        "        \"waiting\", \"login\", \"net\", \"asp\", \"mailto\", \"signup\", \"sidebar\", \"comment\", \"news\", \"privacy\", \"linkedin\",\n",
        "        \"invoice\", \"error\", \"logout\", \"gov\", \"jsp\", \"tel\", \"signin\", \"header\", \"post\", \"story\", \"policy\", \"pinterest\",\n",
        "        \"billing\", \"success\", \"register\", \"edu\", \"css\", \"news\", \"signout\", \"banner\", \"thread\", \"update\", \"conditions\",\n",
        "        \"youtube\", \"shipping\", \"failure\", \"sign up\", \"co\", \"js\", \"irc\", \"user\", \"ad\", \"forum\", \"headline\", \"agreement\",\n",
        "        \"vimeo\", \"payment\", \"retry\", \"sign in\", \"io\", \"json\",\"file\", \"account\", \"advertisement\", \"discussion\", \"media\",\n",
        "        \"license\", \"flickr\", \"subscribe\", \"refresh\", \"sign out\", \"uk\", \"xml\", \"profile\", \"promo\", \"like\",\n",
        "        \"video\", \"cookie\", \"reddit\", \"membership\", \"reload\", \"contact\", \"de\", \"pdf\", \"href\", \"admin\", \"button\",\n",
        "        \"share\", \"image\", \"settings\", \"tumblr\", \"account\", \"redirect\", \"about\", \"jp\", \"doc\", \"src\", \"dashboard\",\n",
        "        \"click\", \"follow\", \"photo\", \"preferences\", \"snapchat\", \"profile\", \"navigate\", \"home\", \"fr\", \"docx\", \"ref\",\n",
        "        \"settings\", \"read more\", \"subscribe\", \"gallery\", \"options\", \"tiktok\", \"wishlist\", \"submit\", \"menu\", \"au\",\n",
        "        \"xls\", \"utm_source\", \"search\", \"more info\", \"unsubscribe\", \"slideshow\", \"tools\", \"whatsapp\", \"product\", \"validate\",\n",
        "        \"search\", \"ca\", \"xlsx\", \"utm_medium\", \"results\", \"next\", \"notification\", \"podcast\", \"utilities\", \"telegram\",\n",
        "        \"service\", \"authenticate\", \"next\", \"us\", \"ppt\", \"utm_campaign\", \"help\", \"previous\", \"alert\", \"episode\",\n",
        "        \"resources\", \"messenger\", \"pricing\", \"authorize\", \"previous\", \"in\", \"pptx\", \"utm_term\", \"support\", \"back\",\n",
        "        \"update\", \"stream\", \"links\", \"skype\", \"offer\", \"encrypt\", \"back\", \"cn\", \"txt\", \"utm_content\", \"about\", \"top\",\n",
        "        \"upload\", \"broadcast\", \"map\", \"discord\", \"discount\", \"help\", \"br\", \"zip\", \"param\", \"home\", \"bottom\", \"attachment\",\n",
        "        \"channel\", \"navigation\", \"signal\", \"coupon\", \"support\", \"es\", \"rar\", \"sid\", \"news\", \"skip\", \"link\", \"playlist\",\n",
        "        \"sitemap\", \"medium\", \"gift\", \"terms\", \"tar\", \"id\", \"blog\", \"submit\",\"url\", \"archive\", \"blogspot\", \"buy\",\n",
        "        \"conditions\", \"gz\", \"key\", \"post\", \"reset\", \"address\", \"library\", \"wordpress\", \"sell\", \"privacy\", \"exe\", \"token\",\n",
        "        \"article\", \"cancel\", \"contact\", \"resource\", \"github\", \"rent\", \"policy\", \"dmg\", \"hash\", \"category\", \"edit\",\n",
        "        \"phone\", \"bitbucket\", \"lease\", \"disclaimer\", \"iso\", \"index\", \"tag\", \"delete\", \"email\", \"stackoverflow\", \"booking\",\n",
        "        \"sitemap\", \"bin\", \"page\", \"archive\", \"save\", \"support\", \"quora\", \"reservation\", \"faq\", \"img\", \"sort\", \"year\",\n",
        "        \"print\", \"help\", \"meetup\", \"feedback\", \"filter\", \"month\", \"close\", \"faq\", \"eventbrite\", \"news\", \"author\",\n",
        "        \"collapse\", \"tutorial\", \"blog\", \"faq\", \"dropdown\", \"documentation\", \"post\", \"terms\", \"form\", \"manual\",\n",
        "        \"article\", \"privacy\", \"field\", \"report\", \"read more\", \"policy\", \"checkbox\", \"feedback\", \"follow us\",\n",
        "        \"conditions\", \"radio\", \"survey\", \"share\", \"disclaimer\", \"select\", \"like\", \"sitemap\", \"option\",\n",
        "        \"comment\", \"feedback\", \"input\", \"subscribe\", \"gallery\", \"text area\", \"unsubscribe\", \"media\", \"captcha\",\n",
        "        \"learn more\", \"video\", \"view details\", \"image\", \"all rights reserved\", \"photo\", \"© (copyright)\", \"download\",\n",
        "        \"trademark\", \"upload\", \"update\", \"file\", \"settings\", \"attachment\", \"profile\", \"resource\", \"account\", \"link\",\n",
        "        \"admin\", \"share\", \"dashboard\", \"my account\", \"your account\", \"preferences\", \"notifications\", \"messages\", \"inbox\",\n",
        "        \"outbox\", \"send\", \"receive\", \"cart\", \"checkout\", \"order\", \"payment\", \"invoice\", \"billing\", \"shipping\", \"address\",\n",
        "        \"terms of service\", \"conditions of use\", \"user agreement\", \"cookies\", \"advertisement\", \"sponsor\", \"partnership\",\n",
        "        \"careers\", \"jobs\", \"vacancies\", \"apply now\", \"application\", \"newsletter\", \"updates\", \"events\", \"calendar\",\n",
        "        \"press\", \"release\", \"media\", \"gallery\", \"videos\", \"photos\", \"terms & conditions\", \"privacy & policy\", \"contact us\",\n",
        "        \"back to top\", \"accessibility\", \"languages\", \"select language\", \"international\", \"mobile\", \"desktop\"\n",
        "    ]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Update the stopwords list with company-specific stopwords\n",
        "    stop_words.update(company_stopwords)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgEdphRtErjy",
        "outputId": "e47e8dda-cb4a-442c-d5b6-b2086e797c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the company website URL: https://www.tcs.com/\n",
            "Error reading tsc-omnistore-ai-powered-unified-composable-commerce-platform.pdf: No /Root object! - Is this really a PDF?\n",
            "Error reading report-tcs-ai-for-business-study.pdf: Invalid dictionary construct: [/'I', False, /'K', /b'fa', /b'lse', /'S', /'Transparency', /'Type', /'Group']\n",
            "Processed text saved to tcs_scraped_data/processed_text_from_pdfs.txt\n",
            "All content extracted and combined into tcs_scraped_data/processed_html_content.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8809f3355afd>:247: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(html_content, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All content extracted and combined into tcs_scraped_data/processed_Careers_content.txt\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    company_url = input(\"Enter the company website URL: \").strip('\\'\"')\n",
        "    parsed_url = urlparse(company_url)\n",
        "    domain = parsed_url.netloc.split('.')[1] if 'www' in parsed_url.netloc else parsed_url.netloc.split('.')[0]\n",
        "    output_folder = f'{domain}_scraped_data'\n",
        "    common_tabs = ['#main-nav', '.footer', 'header', 'footer']\n",
        "    Careers = f'{domain}_careers_scraped_data'\n",
        "\n",
        "    # Step 1: Scrape the website and career sections\n",
        "    create_folder(output_folder)\n",
        "    scrape_website(company_url, output_folder, common_tabs=common_tabs)\n",
        "    scrape_careers_section(company_url)\n",
        "\n",
        "    # Step 2: Process PDFs from the documents folder\n",
        "    documents_folder = os.path.join(output_folder, 'documents')\n",
        "    pdf_output_file = os.path.join(output_folder, 'processed_text_from_pdfs.txt')\n",
        "    process_pdfs(documents_folder, pdf_output_file)\n",
        "\n",
        "    # Step 3: Combine HTML content into a single text file\n",
        "    html_folder = os.path.join(output_folder, 'htmls')\n",
        "    combined_html_file = os.path.join(output_folder, 'processed_html_content.txt')\n",
        "    process_html_files_in_directory(html_folder, combined_html_file)\n",
        "\n",
        "    #Step 4: Combine Careers content into a single text file\n",
        "    Careers_folder = os.path.join(output_folder, Careers)\n",
        "    combined_Careers_file = os.path.join(output_folder, 'processed_careers_content.txt')\n",
        "    process_html_files_in_directory(Careers_folder, combined_Careers_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq1Xbpm4hYEo",
        "outputId": "323e403a-2b7b-4f14-cf88-337ddef6ceb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkVz0KXFhgcB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}