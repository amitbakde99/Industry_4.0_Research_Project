{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version enables to scrape all documents & full htmls from any website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4eb049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the company website URL: https://www.tcs.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import hashlib\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress SSL certificate warnings (use with caution)\n",
    "requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    \"\"\"Creates a folder if it doesn't exist.\"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "def get_unique_filename(url):\n",
    "    \"\"\"Generates a unique filename based on URL hash.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    file_name = hashlib.md5(url.encode()).hexdigest()\n",
    "    return file_name + \".html\"\n",
    "\n",
    "def get_document_filename(url):\n",
    "    \"\"\"Extracts the filename from the URL for documents.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    return os.path.basename(parsed_url.path)\n",
    "\n",
    "def save_content(content, folder_path, filename):\n",
    "    \"\"\"Saves content (text or binary) to a file.\"\"\"\n",
    "    with open(os.path.join(folder_path, filename), 'wb') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def scrape_website(url, folder_path, visited=set(), depth=0, max_depth=3, common_tabs=None):\n",
    "    \"\"\"Recursively scrapes a website up to max_depth.\"\"\"\n",
    "    if depth > max_depth or url in visited:\n",
    "        return\n",
    "\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        if common_tabs:\n",
    "            for tab in common_tabs:\n",
    "                for elem in soup.select(tab):\n",
    "                    elem.decompose()\n",
    "\n",
    "        # Save HTML content excluding header, footer, and common tabs\n",
    "        html_folder = os.path.join(folder_path, 'htmls')\n",
    "        create_folder(html_folder)\n",
    "        filtered_content = soup.prettify('utf-8')\n",
    "        save_content(filtered_content, html_folder, get_unique_filename(url))\n",
    "\n",
    "        # Save documents\n",
    "        doc_folder = os.path.join(folder_path, 'documents')\n",
    "        create_folder(doc_folder)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            full_url = urljoin(url, href)\n",
    "            if urlparse(full_url).netloc == urlparse(url).netloc:\n",
    "                if full_url.endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx')):\n",
    "                    save_content(requests.get(full_url, verify=False).content, doc_folder, get_document_filename(full_url))\n",
    "                else:\n",
    "                    scrape_website(full_url, folder_path, visited, depth + 1, max_depth, common_tabs)\n",
    "                    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    company_url = input(\"Enter the company website URL: \")\n",
    "    parsed_url = urlparse(company_url)\n",
    "    domain = parsed_url.netloc.split('.')[1] if 'www' in parsed_url.netloc else parsed_url.netloc.split('.')[0]\n",
    "    output_folder = f'{domain}_scraped_data'\n",
    "    common_tabs = ['#main-nav', '.footer', 'header', 'footer']\n",
    "\n",
    "    create_folder(output_folder)\n",
    "    scrape_website(company_url, output_folder, common_tabs=common_tabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46418811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915f391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
